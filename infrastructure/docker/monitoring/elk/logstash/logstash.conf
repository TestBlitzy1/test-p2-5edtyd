# Logstash Configuration for Sales Intelligence Platform
# Version: 7.17.x
# Purpose: Centralized log aggregation and processing with secure inputs and optimized performance

# Performance and Queue Settings
pipeline.workers: 2
pipeline.batch.size: 125
pipeline.batch.delay: 50
queue.type: persisted
queue.max_bytes: 1gb
queue.checkpoint.writes: 1024
path.queue: /var/lib/logstash/queue
path.dead_letter_queue: /var/lib/logstash/dlq
dead_letter_queue.enable: true

input {
  # Filebeat Input for Microservices Logs
  beats {
    port => 5044
    host => "0.0.0.0"
    client_inactivity_timeout => 60
    ssl_enabled => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    tags => ["filebeat"]
  }

  # TCP Input for JSON Formatted Logs
  tcp {
    port => 5000
    codec => json
    ssl_enable => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    tags => ["tcp"]
  }

  # HTTP Input for Direct Log Ingestion
  http {
    port => 8080
    codec => json
    ssl_enable => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    tags => ["http"]
  }
}

filter {
  # Custom Grok Patterns for Structured Log Parsing
  grok {
    patterns_dir => "/usr/share/logstash/patterns"
    pattern_definitions => {
      "CAMPAIGN_ID" => "[A-Z0-9]{8}"
      "METRIC_TYPE" => "(CTR|CPC|CPM|ROAS)"
      "PERFORMANCE_VALUE" => "\d+\.?\d*"
    }
  }

  # Standardized Timestamp Processing
  date {
    match => ["timestamp", "log_timestamp", "@timestamp"]
    target => "@timestamp"
    timezone => "UTC"
  }

  # Log Enrichment with Operational Metadata
  mutate {
    add_field => {
      "environment" => "${ENV:production}"
      "platform" => "sales-intelligence"
      "service_version" => "${SERVICE_VERSION}"
      "cluster_name" => "${CLUSTER_NAME}"
      "region" => "${AWS_REGION}"
    }
  }

  # Main Pipeline Processing
  if "filebeat" in [tags] {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] %{GREEDYDATA:log_message}" }
    }
  }

  # Metrics Pipeline Processing
  if [type] == "metric" {
    grok {
      match => { "message" => "%{CAMPAIGN_ID:campaign_id} %{METRIC_TYPE:metric_type} %{PERFORMANCE_VALUE:value}" }
    }
    mutate {
      convert => { "value" => "float" }
    }
  }
}

output {
  # Primary Elasticsearch Output
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}"]
    index => "sales-intelligence-logs-%{+YYYY.MM.dd}"
    user => "${ELASTICSEARCH_USERNAME}"
    password => "${ELASTICSEARCH_PASSWORD}"
    ssl_enabled => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"
    ilm_enabled => true
    ilm_rollover_alias => "sales-intelligence"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "sales-intelligence-policy"
  }

  # Metrics-Specific Output
  if [type] == "metric" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}"]
      index => "sales-intelligence-metrics-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USERNAME}"
      password => "${ELASTICSEARCH_PASSWORD}"
      ssl_enabled => true
      ssl_certificate_verification => true
      cacert => "/etc/logstash/certs/ca.crt"
      ilm_enabled => true
      ilm_rollover_alias => "sales-intelligence-metrics"
      ilm_pattern => "{now/d}-000001"
      ilm_policy => "sales-intelligence-metrics-policy"
    }
  }

  # Dead Letter Queue for Failed Events
  if [@metadata][dead_letter_queue] {
    file {
      path => "/var/lib/logstash/dlq/%{+YYYY-MM-dd}-failed-events.log"
      codec => json
    }
  }
}